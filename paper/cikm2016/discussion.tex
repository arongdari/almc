%!TEX root = ./cikm2016.tex
\section{Discussion}
We have proposed a novel compositional relational model with uncertainty and presented the
Thompson sampling for both compositional and non-compositional models to solve both knowledge completion and active knowledge acquisition problems.
The compositional model aims to infer the latent features of knowledge 
bases by incorporating an additional graph structure. In the passive 
learning scenario, the compositional model outperforms the other models, 
especially, when training size is relatively small. 
In the active learning scenario, probabilistic RESCAL achieves the highest 
cumulative gain across all datasets. Again, this result emphasise the 
importance of being balanced between exploration and exploitation. 

Previous work such as the one by \cite{kajino2015active} 
views knowledge population 
and knowledge completion are separate problems. 
We find this observation true when the algorithm has a warm-start, 
i.e. already having a fair amount of data before active learning starts; 
when the information is sparse, the same strategy works for both maximising 
recall and reducing uncertainty.  
\eat{This finding
may hold for some scenarios where their is already a proper amount of 
information to exploit the structure we want to maximise, but the finding 
is not consistence in more conservative cases.}
Thompson sampling has been studied in the context of multi-armed bandit 
problems where the goal is to maximise cumulative gains or minimise cumulative 
regrets over time, whereas its performance on making a predictive model has not 
been widely discussed so far. Its performance on building a generalisable model 
was unclear. Throughout this work, we have empirically shown that maximising 
cumulative gain entails the predictive models as well.
In the long run, we see this work as a promising step towards using a composition-aware knowledge 
completion system to connect with the 
knowledge based construction problem~\cite{dong2014knowledge}. %, with rich graph structures. 
