%!TEX root = ./cikm2016.tex

\section{Experiments on incremental\\ Knowledge Population}
\label{sec:exp2}

In this section, we show results for Thomson sampling of the \textsc{Prescal} and its compositional variants on the three datasets. In the appendix, we also measure a validity of our Thompson sampling algorithm on two synthetic datasets.
%\rev{These experiments compare exploitation and exploration
% with exploitation only algorithms, and also show how the compositional model
%improves the entity embedding upon the non-compositional models. }
%%LX: second sentence can be cut if we need space.

\begin{figure*}[t]
	\centering

	\subfigure[KINSHIP]{
	\includegraphics[width=0.295\linewidth]{images/thompson_kinship_mcmc_vertical_line.pdf}
	}
	\subfigure[UMLS]{
	\includegraphics[width=0.295\linewidth]{images/thompson_umls_mcmc_vertical_line.pdf}
	}
	\subfigure[NATION]{
	\includegraphics[width=0.295\linewidth]{images/thompson_nation_mcmc_vertical_line.pdf}
	}
	\caption{\label{fig:vs_greedy} The cumulative gain and ROC-AUC score of the Thompson sampling with passive learning and  AMDC models. Thompson sampling with PRESCAL (\textsc{PNORMAL-TS}) model achieves the highest cumulative gain to compare with the other models and shows comparable performance on ROC-AUC scores.}
\end{figure*}

\eat{
\begin{figure}[t]
	\centering
	\subfigure[\scriptsize Logistic: N=10, K=10, D=5\label{fig:syn1}]{
	\includegraphics[width=0.43\linewidth]{images/toy_logit_vs_normal_10_10_5.pdf}
	}
	\subfigure[\scriptsize Logistic: N=20, K=10, D=5\label{fig:syn2}]{
	\includegraphics[width=0.45\linewidth]{images/toy_logit_vs_normal_20_10_5.pdf}
	}
	\subfigure[\scriptsize Gaussian: N=10, K=10, D=5\label{fig:syn3}]{
	\includegraphics[width=0.45\linewidth]{images/toy_10_10_5.pdf}
	}
	\subfigure[\scriptsize Gaussian: N=20, K=10, D=5\label{fig:syn4}]{
	\includegraphics[width=0.45\linewidth]{images/toy_20_10_5.pdf}
	}
%	\includegraphics[width=0.32\linewidth]{images/toy_5_10_5.pdf}
%	\includegraphics[width=0.32\linewidth]{images/toy_10_5_5.pdf}
	\caption{\label{fig:synthetic} Cumulative regret of particle Thompson sampling with Gaussian and logistic output (\textsc{Pnormal-ts}, PLOGIT-TS) against Passive learning
	on synthetic datasets with logistic	(top row, a, b) and Gaussian (bottom row, c, d) output variables.
	%We compared the particle Thompson sampling with passive sampling method.
	The averaged cumulative regrets over 10 runs are plotted with one standard error.
	As the model obtained more and more labeled samples from Thompson sampling,
	the cumulative regrets increase sub-linearly.}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{images/toy_comp_5_2_5.pdf}
	\caption{\label{fig:comp_synthetic} Cumulative regret of particle Thompson sampling of the compositional models on synthetic dataset with N=5, D=5. The synthetic dataset has three relations (K=3); the first two are independently generated, and the third relation is composed by the first two relations. The dataset used in (a) is generated by the multiplicative assumption, and the dataset used in (b) is generated by the additive assumption.}
\end{figure}

\subsection{Thompson Sampling on synthetic data}

We first synthesise two datasets
following the model assumptions in Equation \ref{eqn:entity_gen} to
\verify{\ref{eqn:triple_gen}}.
\eat{We randomly generate triples based on randomly sampled entities and relations. Every }
\rev{First, }entities and relations are generated from zero-mean isotropic multivariate normal distribution, \eat{where we set }\rev{with }
variance parameters $\sigma_e=1$, $\sigma_r=1$, respectively.
\rev{We generate two sets of \eat{the final}output triples,}
\eat{we generate two sets of datasets with }
\rev{with} the logistic output \eat{output variable }and the Gaussian \rev{with $\sigma_x$ set to 0.1}\eat{ variable}, respectively (Sec~\ref{sec:brescal}).
\eat{The variance of gaussian $\sigma_x$ set to 0.1.}

To measure performance\eat{ on synthetic dataset}, we compute cumulative regret
\eat{of proposed algorithm }at each time $n$ as $R(n) = \sum_{t=1}^{n} x_t - x^{*}_t$,
%\begin{align}
%R(n) = \sum_{t=1}^{n} x_t - x^{*}_t,
%\end{align}
where $x^*_t$ is the highest\rev{-valued} triple among triples that have not been chosen up to time $t$. Unlike the general
bandit setting where one can select a single item multiple times, in our formulation, we can select one triple
only once. So after selecting a triple at time $t$, the selected triple will be removed from a set of candidate
triples.

Figure \ref{fig:synthetic} shows the cumulative regret of the algorithm on the synthetic data with varying size of
entities and relations. We compare the cumulative regret of the particle Thompson sampling with the passive
learning method where the model choose a random triple at each time. All results are averaged over 10
individual runs with different initialisations.
Note that the dataset with binary logistic output variables can be used to train both logistic-output \textsc{Prescal} (PLOGIT) and Gaussian-output \textsc{Prescal} (PNORMAL) whereas the dataset with the Gaussian output can only be trained by PNORMAL.
Figure \ref{fig:syn1} and \ref{fig:syn2} show that with the logistic synthetic dataset both models are capable to learn the latent features of the generated triples, with logistic outperforming the Gaussian; Figure \ref{fig:syn3} and \ref{fig:syn4} show that the Thompson sampling for PNORMAL (\textsc{Pnormal-ts}) \rev{outperform passive learning in} the real valued dataset.
%For every experiment, the cumulative regret of the Thompson
%sampling method was bounded after a certain number of interactions whereas the cumulative regret of
%passive learning increases linearly.
\eat{Both results indicate the particle sampling is capable of inferring latent features
of entities and relations as the interaction increases.}

\subsection{Thompson sampling for compositional\\ models on synthetic data}

We conduct a second experiment on synthetic dataset to understand how
the Thompson sampling works for the compositional data.
As in the first experiment, we first generate entities and relations from
zero-mean multivariate normal with variance parameter $\sigma_e = 1$ and
$\sigma_r=1$. We generate a set of triples with Gaussian output as in
Equation \ref{eqn:triple_gen}. We then synthesise two sets of expanded tensors
using the previously used entities and relations based on the multiplicative
and additive compositional assumptions, defined in Sec \ref{sec:comp},
respectively. So we synthesise fully observable expanded tensor $\mathcal{X}^L$
where $L=2$. We set both variance parameter $\sigma_x$ and $\sigma_c$ to 0.1.
Note that in a real world situation, the expanded tensor can only be constructed
through the observed triples, and the triples in the expanded tensor cannot be queried.

To run the particle Thompson sampling on the synthetic dataset, we let the
compositional models know which relation is composed by other relations.
The non-compositional PNORMAL model assumes each relation is independent to one another.
Therefore, the compositional model uses much less number of parameters to model
the same size of tensor to compare with the non-compositional model.
\eat{%%LX: these info repeats?
Through
this experiments, we verify the particle Thompson sampling algorithm for the
compositional models.

For the experiment, we generate three relations ($K=3$) of five entities ($N=5$):
the first and second relations
are independent and the third relation is composed by the first and second
relations. The latent dimension is set to 5.
}
With this fully observable expanded tensors, we run the Thompson sampling of
the compositional models.
Figure \ref{fig:comp_synthetic} shows the cumulative regrets on synthetic
datasets. The multiplicative and additive compositionality are used to
generate the dataset for Figure \ref{fig:comp_synthetic}(a) and
\ref{fig:comp_synthetic}(b), respectively. The results correspond to our
assumption: the Thompson sampling for multiplicative compositional model (BCOMP-MUL-TS) shows lower
regrets on the multiplicative data in Figure \ref{fig:comp_synthetic}(a), and
the Thompson sampling for additive compositional model (BCOMP-ADD-TS) shows lower regrets on the
additive compositional data in Figure \ref{fig:comp_synthetic}(b),
and both have lower regrets than passive learning or \textsc{Pnormal-ts} without compositions.
}

% \subsection{Thompson sampling on real datasets}
%
% Next, we evaluate particle Thompson sampling for both compositional and non-compositional models on real datasets.

\textbf{Experimental settings}:
We compare the Thompson sampling models with \textsc{Amdc} models, and \textsc{Prescal} for passive learning.
\textsc{Amdc} model has been proposed to achieve two different active learning goals, constructing a predictive
model and maximising the valid triples in a knowledge base, with two different querying strategies
~\cite{kajino2015active}.
\textsc{Amdc-pred} is a predictive model construction strategy and chooses a triple which is the most ambiguous (close to the decision boundary) at each time $t$.
\textsc{Amdc-pop} is a population strategy which aims to maximise the number of valid triples in a knowledge base, choosing a triple with the highest expected value at each time.
To train all models we only use the observed triples up to the current time.
For the passive learning with \textsc{Prescal}, we generate a random sample at each time period.
For the particle Thompson sampling models, we set variance parameter $\sigma_e$ and $\sigma_r$ to 1, $\sigma_x$ to 0.1, and vary $\sigma_c$ from 1 to 100.

We leave 30 \% of triples as a test set to measure test error.
At each time period, each model chooses one triple to query,
if the selected triple is in the test set then we choose the next highest expected triple which is not in the test set.
All models start from zero observation.
After every querying, a model obtains a label of the queried triple from an oracle,
then the model updates the parameters.

\textbf{Evaluation metric}: We use two different evaluation metrics, cumulative gain and ROC-AUC score,
for the performance comparison. The goal of the Thompson sampling is to maximise the knowledge
population through the balanced querying strategy between exploration and exploitation.
To measure how many triples are obtained through the querying stage, we compute the cumulative
gain which is the number of valid triple obtained up to time $t$. Additionally, we compute the ROC-AUC score on
the test set to understand how this balanced querying strategy results in making a predictive model.

\textbf{Exploitation and exploration}:
Figure \ref{fig:vs_greedy} shows
the cumulative gains and ROC-AUC scores of the Thompson sampling on three real datasets.
\textsc{Pnormal-ts} performs better than other baseline models for the cumulative gain, and shows comparable result for the ROC-AUC scores. Both compositional models perform worse than \textsc{Pnormal-ts} across all datasets.

In the original \textsc{Amdc} \cite{kajino2015active}, \textsc{Amdc-pop} model obtains more
valid triples than \textsc{Amdc-pred}, and \textsc{Amdc-pred} shows high ROC-AUC scores than \textsc{Amdc-pop}.
In our experiment, however, \textsc{Amdc-pop} shows comparable cumulative gain to \textsc{Amdc-pred}
and even worse than \textsc{Amdc-pred} for the UMLS. We conjecture the initial observation and query size results in the different performances: in the original experiment, the model starts
from a small set of training data, and the query size was 1,000 for KINSHIP and UMLS. So this gives the model focusing on exploit and advantage,
%a certain latent structure given an initially trained model
whereas in our experiment, we start from zero
observation and query one triple at each time, which makes the model hard to exploit the structure. This result shows
the importance of balancing between exploitation and exploration.

\begin{figure}[t]
	\centering

%	\subfigure[KINSHIP]{
%	\includegraphics[width=0.9\linewidth]{images/posterior_variance_trace_kinship.pdf}
%	}
%	\subfigure[UMLS]{
%	\includegraphics[width=0.9\linewidth]{images/posterior_variance_trace_umls.pdf}
%	}
%	\subfigure[NATION]{
%	\includegraphics[width=0.9\linewidth]{images/posterior_variance_trace_nation.pdf}
%	}

	\includegraphics[width=0.9\linewidth]{images/posterior_variance_trace_kinship.pdf}

	\caption{\label{fig:pos_var} Trace plot of mean posterior variance of the non-compositional model and compositional models. Y-axis denotes the average posterior covariance, and X-axis denotes the number of queries. The second plot magnifies the first plot.}
\end{figure}

We note that the compositional model performs worse than the non-compositional models,
especially than \textsc{Pnormal-ts}.
This is counter-intuitive to our general understanding where
the model that performs well in the predictive task also shows
a better performance in the active learning.
Of course, we also emphasise the difference between two experiments;
the goal of incremental population is to maximise the number of triples
whereas the goal of knowledge completion in Section \ref{sec:exp1} is to maximise
the predictive performance. Nevertheless, the compositional models do not outperform
\textsc{Pnormal-ts} in the active learning.
This result can be partially understood in terms of the balance between
exploration-exploitation. Figure \ref{fig:pos_var} shows the average posterior variance of
the entity vectors. We compute the eigenvalues of posterior covariance matrix $\Lambda_i^{-1}$
and trace the average eigenvalues over the iterations.
As shown in the figure, the average variance of the compositional model shrinks much faster
than the \textsc{Pnormal-ts}. Because the exploration-exploitation of the Thompson sampling depends on the
posterior uncertainty, the fast shrinkage in the posterior variance may indicate the under
exploration of the model. This is predictable to a certain extent in the sense that one new triple with the compositional models induces multiple new
observations in the compositional triples, so the uncertainties of entities and
relations are measured less than those with non-compositional model. The most of active
learning algorithm utilise an uncertainty of a model, and therefore, a model with augmented
structures such as the relation compositions should be more careful about reflecting its uncertainty correctly.

\eat{If the samples of the compositional model follows the posterior
distribution, it may show the similar performance with the passive learning
scenario, but this assumption does not always hold since the active sampling
path does not correspond to the passive scenario. } %%LX: this looks like too much detail
\eat{
One possible explanation is
that the naive particle Thompson sampling may not capture the posterior of the
complex compositional structure, so the particle degenerates over time. Recent
 advances in sequential Monte Carlo may help to solve the problem\cite{gu2015neural,naesseth2014sequential,lindsten2014divide}.
We leave this for future work.
}
