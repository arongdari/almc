%!TEX root = ./cikm2016.tex
\section{Probabilistic RESCAL}
\label{sec:brescal}

\begin{table*}[bt]
\caption{Parameters for Gibbs updates. The conditional of $e_i$ and $R_k$ follows the normal distribution with mean $\mu$ and precision matrix $\Lambda$. $\otimes$ is the Kronecker product.}
\label{tab:brescalposterior}
\vskip 0.05in
\begin{tabu}{l|l|l|l}
var&$\mu$&$\Lambda$&$\xi$\\
\hline
$e_i$&
$\frac{1}{\sigma_x^2}\Lambda_i^{-1}\xi_i$&
$\frac{1}{\sigma_x^2} \sum_{jk : x_{ikj} \in \mathcal{X}^{t}} (R_k e_j)(R_k e_j)^\top$&
$\sum_{jk : x_{ikj} \in \mathcal{X}^{t}}  x_{ikj} R_{k} e_{j} +
\sum_{jk : x_{jki} \in \mathcal{X}^{t}} x_{jki} R_{k}^\top e_{j}.$
\\
$vec(R_k)$&
$\frac{1}{\sigma_x^2}\Lambda_k^{-1}\xi_k$&
$\frac{1}{\sigma_x^2} \sum_{ij:x_{ikj} \in \mathcal{X}^{t}} (e_i
\otimes e_j)(e_i \otimes e_j)^\top + \frac{1}{\sigma_r^2} {I}_{D^2}$&
$\sum_{ij:x_{ikj} \in \mathcal{X}^{t}} x_{ikj} (e_{i} \otimes e_{j}).$
\end{tabu}
\end{table*}


A relational knowledge base consists of a set triples in the form of $(i, k, j)$
where $i$, $j$ are entities, and $k$ is a relation. A triple can be distinguished
in a valid triple and invalid triple based on a semantic meaning of the triple. An
example of the valid triple in Freebase is \texttt{(BarackObama, PresidentOf, U.S.)}, and an
example of the invalid triple is \texttt{(BarackObama, PresidentOf, U.K.)}.
A knowledge base can be represented in a three-way binary tensor
$\mathcal{X} \in \{0, 1\}^{N \times K \times N}$, where $K$ is a number of
relations, $N$ is a number of entities, and $x_{ikj}\in \{0, 1\}$ indicates whether
the triple is valid.

We model the entity $i$ as vectors $e_i$ and the relation $k$ as matrix $R_k$ with an
appropriately chosen latent dimension $D$. This follows a popular model
for statistical relational learning, which is to factorise the tensor into a
set of latent vector representations, such as the bilinear model RESCAL~\cite{nickel2011three}.
RESCAL aims to factorise each relational slice $X_{:k:}$ into a set of rank-$D$ latent
features as follows:
\[
  \mathcal{X}_{:k:} \approx E R_k E^\top, \qquad \text{for } k = 1, \dots, K
\]
Here, $E\in {\mathbb R}^{N \times D}$ contains the latent features of the
entities $e_1, \ldots, e_N$ and $R_k\in {\mathbb R}^{D \times D}$ models the interaction of the
latent features between entities in relation $k$.

We propose a probabilistic framework that directly generalises RESCAL
by placing priors over the
latent features. For each entity $i$, the latent feature of an entity $e_i \in
\mathbb{R}^{D}$ is drawn from an isotropic multivariate-normal distribution.
\begin{align}
\label{eqn:entity_gen}
e_i \sim {N}(\mathbf{0}, \sigma_e^2{I}_D)
\end{align}
For each relation $k$, we draw matrix $R_k$ from
a zero-mean isotropic matrix normal distribution.
\begin{align}
\label{eqn:relation_gen}
R_k \sim \mathcal{MN}_{D \times D}(\mathbf{0}, \sigma_r{I}_D, \sigma_r{I}_D) \\
\text{or equivalently}\enspace r_k  = \text{vec}(R_k) \sim N(\mathbf{0}, \sigma_r^2 I_{D^2}) \notag
\end{align}
where $\text{vec}(R_k)$ denotes the flattening of the matrix.

We consider two observation models for $x_{ikj}$: real or binary variables. By placing a
normal distribution over $x_{ikj}$,
\begin{align}
  x_{ikj} |e_i, e_j, R_k \sim \mathcal{N}(e_i^{\top} R_k e_j, \sigma_x^2),\label{eqn:triple_gen}
\end{align}
we model the value of triple as a real variable.
This is not a natural choice since the triple is a binary variable, however, we can control the confidence on different observations
through the variance parameter $\sigma_x^2$.
%The role of this parameter will be further discussed in the compositional model section.

We develop a Gibbs sampler to perform the posterior inference for the probabilistic RESCAL (PRESCAL). The conditional distribution of each latent variable is given by:
\begin{align}
p(e_i |E_{-i}, \mathcal{R}, \mathcal{X}^{t}, \sigma_e, \sigma_x) &= \mathcal{N}(e_i | \mu_i,
\Lambda_i^{-1})  \label{eqn:sample_e} \\
p(R_k|E, \mathcal{X}, \sigma_r, \sigma_x) &= \mathcal{N}(\text{vec}(R_k) |
\mu_k, \Lambda_k^{-1}) \label{eqn:sample_r}
\end{align}
where the negative subscript $-i$ indicates the every other entity variables except entity $i$.
Exact forms of the posterior means and precision matrices are listed in Table~\ref{tab:brescalposterior}, where we have
used the identity $e_i^{\top} R_k e_j = r_k^{\top} e_i \otimes e_j$.

Alternatively, we may want to model the binary observation more precisely.
Therefore we model $x_{ikj}$ as a binomial random variable whose
probability is determined by logistic regression:
\[
p(x_{ikj}=1) = \sigma(e_i^{\top} R_k e_j),
\]
where $\sigma$ is a sigmoid function.
We approximate the conditional posterior of
$E$ and $R$ by Laplace approximation~\cite{bishop2006pattern}. The maximum a
posterior estimate of $e_i$ or $R_k$ given the rest can be computed through 
standard logistic regression solvers with the priors over $e_i$ and $R_k$ as regularisation parameters. Given the
maximum a posteriori parameters $e_i^*$, the posterior covariance $S_i$ of entity
$i$ takes the form
\begin{align*}
S_i^{-1} = \sum_{x_{ikj}} \sigma(e_{i}^{*\top} R_k e_{j}) (1 - \sigma(e_{i}^{*\top} R_k e_{j})) R_k
e_{j}(R_k e_{j})^\top\\
 + \sum_{x_{jki}} \sigma(e_{j}^{\top} R_k e_{i}^*) ( 1- \sigma(e_{j}^{\top} R_k e_{i}^*)) R_k^\top e^*_{i}(R_k^\top e^*_{i})^\top + I\sigma_e^{-1}
.
\end{align*}
The posterior covariance of $R_k$ can be computed in the same way. Let $R_k^*$ is a maximum a posterior solution of $R_k$ given $E$. Then, the conditional posterior covariance $S_k$ of relation $k$ has the form of:
\begin{align}
S_k^{-1} = \sum_{x_{ikj}} \sigma(e_{i}^{\top} R_k^* e_{j}) (1 - \sigma(e_{i}^{\top} R_k^* e_{j})) 
\bar{e}_{ij}\bar{e}_{ij}^\top + I\sigma_r^{-1}, \notag
\end{align}
where $\bar{e}_{ij} = e_i \otimes e_j$.


The probabilistic view of tensor factorisation has many advantages such as
the quantification of uncertainty by the predictive distribution,
the ability to utilise priors, and
the availability of principled model selection.
We show in the empirical experiments that PRESCAL outperforms standard RESCAL.
