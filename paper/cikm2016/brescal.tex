%!TEX root = ./cikm2016.tex
\section{Probabilistic RESCAL}
\label{sec:brescal}

\begin{table*}[bt]
\caption{Parameters for Gibbs updates. The conditional of $e_i$ and $R_k$ follows the normal distribution with mean $\mu$ and precision matrix $\Lambda$. $\otimes$ is the Kronecker product.}
\label{tab:brescalposterior}
\begin{tabu}{l|l|l|l}
var&$\mu$&$\Lambda$&$\xi$\\
\hline
$e_i$&
$\frac{1}{\sigma_x^2}\Lambda_i^{-1}\xi_i$&
$\frac{1}{\sigma_x^2} \sum_{jk : x_{ikj} \in \mathcal{X}^{t}} (R_k e_j)(R_k e_j)^\top$&
$\sum_{jk : x_{ikj} \in \mathcal{X}^{t}}  x_{ikj} R_{k} e_{j} +
\sum_{jk : x_{jki} \in \mathcal{X}^{t}} x_{jki} R_{k}^\top e_{j}.$
\\
$vec(R_k)$&
$\frac{1}{\sigma_x^2}\Lambda_k^{-1}\xi_k$&
$\frac{1}{\sigma_x^2} \sum_{ij:x_{ikj} \in \mathcal{X}^{t}} (e_i
\otimes e_j)(e_i \otimes e_j)^\top + \frac{1}{\sigma_r^2} {I}_{D^2}$&
$\sum_{ij:x_{ikj} \in \mathcal{X}^{t}} x_{ikj} (e_{i} \otimes e_{j}).$
\end{tabu}
\end{table*}


A relational knowledge graph consists of a set triples in the form of $(i, k, j)$
where $i$, $j$ are entities, and $k$ is a relation. A triple can be distinguished
in a valid triple and invalid triple based on a semantic meaning of the triple. An
example of the valid triple in Freebase is \texttt{(BarackObama, PresidentOf, U.S.)}, and an
example of the invalid triple is \texttt{(BarackObama, PresidentOf, U.K.)}.
A knowledge graph can be represented in a three-way binary tensor
$\mathcal{X} \in \{0, 1\}^{N \times K \times N}$, where $K$ is a number of
relations, $N$ is a number of entities, and $x_{ikj}\in \{0, 1\}$ indicates whether
the triple is valid.

We model the entity $i$ as vectors $e_i$ and the relation $k$ as matrix $R_k$ with an
appropriately chosen latent dimension $D$. This follows a popular model
for statistical relational learning, which is to factorise the tensor into a
set of latent vector representations, such as the bilinear model \textsc{Rescal}~\cite{nickel2011three}.
\textsc{Rescal} aims to factorise each relational slice $\mathcal{X}_{:k:}$ into a set of rank-$D$ latent
features as follows:
\[
  \mathcal{X}_{:k:} \approx E R_k E^\top, \qquad \text{for } k = 1, \dots, K
\]
Here, $E\in {\mathbb R}^{N \times D}$ contains the latent features of the
entities $e_1, \ldots, e_N$ and $R_k\in {\mathbb R}^{D \times D}$ models the interaction of the
latent features between entities in relation $k$.

We propose a probabilistic framework that directly generalises \textsc{Rescal} (\textsc{Prescal})
by placing priors over the
latent features. For each entity $i$, the latent feature of an entity $e_i \in
\mathbb{R}^{D}$ is drawn from an isotropic multivariate-normal distribution with variance $\sigma_e^2$,
\begin{align}
\label{eqn:entity_gen}
e_i \sim {N}(\mathbf{0}, \sigma_e^2{I}_D).
\end{align}
For each relation $k$, we draw matrix $R_k$ from
a zero-mean isotropic matrix normal distribution with variance $\sigma_r^2$,
\begin{align}
\label{eqn:relation_gen}
R_k \sim \mathcal{MN}_{D \times D}(\mathbf{0}, \sigma_r{I}_D, \sigma_r{I}_D) \\
\text{or equivalently}\enspace r_k  = \text{vec}(R_k) \sim N(\mathbf{0}, \sigma_r^2 I_{D^2}) \notag
\end{align}
where $\text{vec}(R_k)$ denotes the flattening of the matrix.

\smallhead{PNORMAL} We consider two observation models for $x_{ikj}$: real or binary variables. By placing a
normal distribution over $x_{ikj}$,
\begin{align}
  x_{ikj} |e_i, e_j, R_k \sim \mathcal{N}(e_i^{\top} R_k e_j, \sigma_x^2),\label{eqn:triple_gen}
\end{align}
we model the value of triple as a real variable.
This is not a natural choice since the triple is a binary variable, however, we can control the confidence on different observations
through the variance parameter $\sigma_x^2$.
%The role of this parameter will be further discussed in the compositional model section.
We develop a Gibbs sampler to perform the posterior inference for the normally distributed observation model. The conditional distribution of each latent variable is given by:
\begin{align}
p(e_i |E_{-i}, \mathcal{R}, \mathcal{X}^{t}, \sigma_e, \sigma_x) &= \mathcal{N}(e_i | \mu_i,
\Lambda_i^{-1})  \label{eqn:sample_e} \\
p(R_k|E, \mathcal{X}, \sigma_r, \sigma_x) &= \mathcal{N}(\text{vec}(R_k) |
\mu_k, \Lambda_k^{-1}) \label{eqn:sample_r}
\end{align}
where the negative subscript $-i$ indicates the every other entity variables except entity $i$.
Exact forms of the posterior means and precision matrices are listed in Table~\ref{tab:brescalposterior}, where we have
used the identity $e_i^{\top} R_k e_j = r_k^{\top} e_i \otimes e_j$.

\smallhead{PLOGIT} One may want to model the binary observation more precisely.
Here, we model $x_{ikj}$ as a Bernoulli random variable whose
probability is determined by logistic regression:
\begin{align}
p(x_{ikj}=1) = \sigma(e_i^{\top} R_k e_j), \label{eqn:logit_triple_gen}
\end{align}
where $\sigma$ is a sigmoid function.
We approximate the conditional posterior of
$E$ and $R$ by the Laplace approximation~\cite{bishop2006pattern} through an alternative sampling. The detailed derivations are provided in Appendix \ref{sec:posterior_logistic}.
\eat{
The maximum a posterior estimate of $e_i$ or $R_k$ given the rest can be computed through
standard logistic regression solvers with the priors over $e_i$ and $R_k$ as regularisation parameters. Given the
maximum a posteriori parameters $e_i^*$, the posterior covariance $S_i$ of entity
$i$ takes the form
\begin{align*}
S_i^{-1} = \sum_{x_{ikj}} \sigma(e_{i}^{*\top} R_k e_{j}) (1 - \sigma(e_{i}^{*\top} R_k e_{j})) R_k
e_{j}(R_k e_{j})^\top\\
 + \sum_{x_{jki}} \sigma(e_{j}^{\top} R_k e_{i}^*) ( 1- \sigma(e_{j}^{\top} R_k e_{i}^*)) R_k^\top e^*_{i}(R_k^\top e^*_{i})^\top + I\sigma_e^{-1}
.
\end{align*}
The posterior covariance of $R_k$ can be computed in the same way. Let $R_k^*$ is a maximum a posterior solution of $R_k$ given $E$. Then, the conditional posterior covariance $S_k$ of relation $k$ has the form of:
\begin{align}
S_k^{-1} = \sum_{x_{ikj}} \sigma(e_{i}^{\top} R_k^* e_{j}) (1 - \sigma(e_{i}^{\top} R_k^* e_{j}))
\bar{e}_{ij}\bar{e}_{ij}^\top + I\sigma_r^{-1}, \notag
\end{align}
where $\bar{e}_{ij} = e_i \otimes e_j$.
}

%The probabilistic view of tensor factorisation has many advantages such as
%the quantification of uncertainty by the predictive distribution,
%the ability to utilise priors, and
%the availability of principled model selection.
%We show in the empirical experiments that \textsc{Prescal} outperforms standard \textsc{Rescal}.
\smallhead{Thompson Sampling} The probabilistic framework allows us to quantify the uncertainty of predictive distribution, which is then used to formulate an active learning algorithm. Specifically, we adopt Thompson sampling for active learning, which finds an optimal trade off between exploitation and exploration during active learning.
Thompson sampling provides a model based query selection process~\cite{li11thompson,scott10bandit}. Let $x_{1:t}$ be a sequence of observed triples up to time $t$, and $\theta$ is an underlying parameter governing the rewards $r$.
Thompson sampling chooses the next action $a$ (triple to label),
according to its probability of having high reward:
\begin{align}
\arg\max_a \int \mathbb{I}\bigg[ \mathbb{E}(r|a, \theta)
= \max_{a'}\mathbb{E}(r|a',\theta) \bigg] p(\theta|x_{1:t-1}) d\theta, \notag
\end{align}
where $\mathbb{I}$ is an indicator function. Note that it is sufficient
to draw a random sample from the posterior instead of computing the integral.

We generalise the idea of a particle Thompson sampling originally proposed in \cite{kawale2015efficient} for a matrix factorisation to the tensor factorisation. The detailed algorithm is provided in Appendix \ref{sec:pts}.
