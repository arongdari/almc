% !TEX TS-program = pdflatexmk
\documentclass{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[a4paper, total={6.5in, 8.5in}, voffset=0.2in, footskip=15pt]{geometry}
\usepackage{amsthm}


\newtheorem{theorem}{Theorem} %This is the example presented in the introduction but it has the additional parameter [section] that restarts the theorem counter at every new section.
\newtheorem{corollary}{Corollary}[theorem] %A environment called corollary is created, the counter of this new environment will be reset every time a new theorem environment is used.
\newtheorem{lemma}[theorem]{Lemma} %In this case, the even though a new environment called lemma is created, it will use the same counter as the theorem environment.

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}%[definition]
\newtheorem{remark}{Remark}

\begin{document}

\section{Learning Knowledge Graph with Pairwise Ranking Objective}
Let $\mathcal{E}$ be a set of entities and $\mathcal{R}$ a set of relations. A knowledge graph $\mathcal{G}$ is a set of positive triples (known facts) which consist of relations between source entity and target entity. Let $s(t)$ be the score function of triple $t = (i, k, j)$ where $i, j \in \mathcal{E}$ and $k \in \mathcal{R}$. The goal of statistical relational learning is to model and infer the score function to correctly classify positive triples among all possible combination of triples.

This is a highly imbalanced and positive-unlabelled classification problem where a number of positive triples is much less than the total number of triples, and the unlabelled triples does not means that those are wrong.
In such a case, we often measure performance using AUC, which measure the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen unlabelled one \cite{menon2011link}. Intuitively, it makes sense to directly optimise for AUC on the training set to obtain good test set AUC.
To maximise AUC on a training set, we can alternatively minimise the following objective:
\begin{align}
\min_s\sum_{t \in \Delta_p}\sum_{t' \in \Delta_u} \mathbf{1}[s(t') - s(t)],
\end{align}
where $\Delta_p$ is a set of known facts (triples), and $\Delta_u$ is a set of unlabelled triples. 

Directly optimising AUC objective is cumbersome. With a proper surrogate loss $\ell$, we can cast the above objective as
\begin{align}
\min_s\sum_{t \in \Delta_p}\sum_{t' \in \Delta_u} \ell(s(t'), s(t)).
\end{align}
For example, the hinge loss was used with an additional margin parameter in \cite{bordes2013translating,Kajino:2015tf}. It can be optimised efficiently using stochastic gradient descent, where we randomly sample a pair of positive and unlabelled triples and compute the gradient at each iteration.

Unlike classification problem where the training set consists of positive and negative instances, uniform sampling of unlabelled triples may not be appropriate since some of them are unknown positive triples. One way to mitigate the weight of `unlabelled positive triple' is to incorporate a weight function $w(t)$ which measure how triple $t$ is likely to be a true negative triple:
\begin{align}
\min_s\sum_{t \in \Delta_p}\sum_{t' \in \Delta_u} w(t') \ell(s(t'), s(t)).
\end{align}
In the context of stochastic gradient descent, instead having the weight function, we may sample a pair of triples from some distribution over the pair of all possible triples. Below, I listed four possible sampling methods.

\begin{example}
The popular RESCAL model defines $s(t_{ikj}) = e_i^\top R_k e_j$, where $e_i, e_j \in \mathbb{R}^{D}$ and $R_k \in \mathbb{R}^{D\times D}$. The TransE model defines $s(t_{ikj}) = d(e_i - r_k, e_j)$, where $e_i, e_j, r_k \in \mathbb{R}^{D}$ and $d$ is some dissimilarity measure. 
\end{example}

\subsection{Sampling unlabelled triple}
\begin{description}
\item[Uniform sampling] Sample $t'$ from $\Delta_u$ uniformly. When the positive triple $t = (i,k,j)$ is given, the most typical choice of sampling distribution is a uniform distribution over the set of unlabelled triples with fixed source (or target) entity $i$ ($j$) and relation $k$, i.e. $\{t':t'=(i,k,j') \text{ or } (i',k,j) \in \Delta_u\}$. This approach does not impose global consistency between different relations, in other words, the ranks among the triples within a single relation is important. For example, triples $(i,k,j)$ and $(i,k',j)$, where $k \neq k'$, will not be directly compared during optimisation.
\item[Maximum unlabelled triple] Given positive triple $(i,k,j)$, one can choose a unlabelled triple whose score is the maximum among all unlabelled triples. The assumption is that all of the unlabelled triples can be distinguished from all of the positive triples. 
\item[Bandit based sampling] Instead choosing maximum unlabelled triple, we can alternatively use Bandit approach to sample. For example, if we can compute the posterior distribution of $s(t')$, we can employ the Thompson sampling, or if we can compute the variance of $s(t')$, we can employ the UCB algorithm.
\item[Modeling distribution] Let $d(t;\theta)$ be a distribution over $\Delta_u$ parameterised by triple $t$. In \cite{Wang2014}, the optimiser choose negative triples based on a type of relation one-to-many/many-to-one/many-to-many. In \cite{Liang2015}, the distribution over items given user is modelled according to topical interests of that user. 
\begin{example}[\cite{Wang2014}]
Given positive triple $(i,k,j)$, let $\mu_t$ be the average number of target entities per source entity in relation $k$, $\mu_s$ be the average number of source entities per target entity in relation $k$. In \cite{Wang2014}, the authors use Bernoulli distribution with parameter $\mu_t/(\mu_s + \mu_t)$ to randomly corrupt source entity $i$, and corrupt source entity $j$ otherwise. They use these quantities to identify the property of relation in terms of one-to-many, many-to-one, and many-to-many, and give more chance to corrupt target entity $j$ if the relation is likely to be many-to-one and to corrupt source entity $i$ otherwise.
\end{example}
\end{description}

\subsubsection{Computational Complexity}
Some of the sampling methods, such as maximum unlabelled triple and bandit based sampling, require to compute some estimates of the score of unlabelled triples where $\arg\max$ or $\arg\min$ operators are applied, i.e. ${\arg\max}_{t}s(t)$. The number of triples to be estimated is $|\mathcal{E}|^2\times|\mathcal{R}|$, which usually cannot be evaluated in a single machine. For example, the subset of freebase used in \cite{gu2015traversing} (FB13) has almost 73 billion possible triples and FB5M has 3e16 possible triples.

\textbf{Type constraints}: Relations are defined with respect to types of source and target entities. If we have types of each entity, we can vastly reduce the number of candidate triples.

\textbf{Approximate argmax}: The number of relations is relatively smaller than the number of entities in general. If it is possible to approximate an entity pair that maximise the score given relation (or entity), we may solve this problem efficiently. In other words, is there a way to approximate ${\arg\max}_{i,j}s((i,k,j))$? (The number of possible entity pairs is still almost 6 billion in the freebase subset.)

Keywords: cutting plane method, Gaussian process approximation, LSH-hashing(?) ...

\subsubsection{Connections to weighted matrix (tensor) factorisation}
If the sum of all positive weights and the sum of all unlabelled weights are the same, i.e. balancing positive and unlabelled triples, does the matrix factorisation reduce to the above objective?

\bibliographystyle{plain}
\bibliography{ref}

\end{document}
