{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Particle Thompson Sampling with Toy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model with synthetic data and compare the cumulative regret of the model with the cumulative regret of random selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "from brescal import gen_random_tensor\n",
    "from seq_brescal import PFBayesianRescal, compute_regret\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Toy Dataset & RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_test = 10\n",
    "var_xs = [0.1, 0.01]\n",
    "mc_moves = [1, 5]\n",
    "n_particles = [5, 10]\n",
    "rbps = [True, False]\n",
    "\n",
    "# toy data generation parameters\n",
    "n_dims = [5, 10]\n",
    "n_entities = [5, 10]\n",
    "n_relations = [5, 10]\n",
    "\n",
    "for n_dim, n_entity, n_relation in itertools.product(n_dims, n_entities, n_relations):\n",
    "    # additional parameters\n",
    "    max_iter = n_relation * n_entity**2\n",
    "    random_regret = np.zeros(max_iter)\n",
    "    result = list()\n",
    "\n",
    "    # dest\n",
    "    dest = '../result/toy/'\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "\n",
    "    pool = mp.Pool(8)\n",
    "    tic=time.time()\n",
    "\n",
    "    T = np.zeros([n_test, n_relation, n_entity, n_entity])\n",
    "    for nt in range(n_test):\n",
    "        # generate toy data\n",
    "        var_e = 1.\n",
    "        var_r = 1.\n",
    "        var_x = 0.01\n",
    "        test_file = os.path.join(dest, 'toy_data__%d_%d_%d_%d_%.2f_%.2f_%.2f' % (nt, n_dim, n_entity, n_relation, var_e, var_r, var_x))\n",
    "        if os.path.exists(test_file):\n",
    "            with open(test_file, 'rb') as f:\n",
    "                _T = pickle.load(f)\n",
    "                T[nt] = _T\n",
    "        else:\n",
    "            _T = gen_random_tensor(n_dim, n_entity, n_relation, var_e=var_e, var_r=var_r, var_x=var_x)\n",
    "            T[nt] = _T\n",
    "            with open(test_file, 'wb') as f:\n",
    "                pickle.dump(_T, f)\n",
    "\n",
    "        _var_x = var_x\n",
    "        # particle thompson sampling\n",
    "        for n_particle, rbp, var_x, mc_move in itertools.product(n_particles, rbps, var_xs, mc_moves):\n",
    "            log_file = os.path.join(dest, 'pThompson_%d_%d_%d_%d_%.2f_%.2f_%.2f_%d_%r_%.2f_%d.txt' % (nt, n_dim, n_entity, n_relation, var_e, var_r, _var_x, n_particle, rbp, var_x, mc_move))\n",
    "            if os.path.exists(log_file):\n",
    "                continue\n",
    "\n",
    "            def finalize(nt, n_particle, rbp, var_x, mc_move):\n",
    "                # function argument closure\n",
    "                def inner(rval):\n",
    "                    return result.append((rval, nt, n_particle, rbp, var_x, mc_move))\n",
    "                return inner\n",
    "\n",
    "            _callback = finalize(nt, n_particle, rbp, var_x, mc_move)\n",
    "\n",
    "            print(log_file)\n",
    "            maskT = np.zeros_like(T[nt])\n",
    "            model = PFBayesianRescal(n_dim, var_x = var_x, controlled_var=False, n_particles=n_particle,\n",
    "                                     compute_score=False, parallel=False, sample_prior=False, \n",
    "                                     gibbs_init=False, rbp=rbp, mc_move=mc_move, log=log_file)\n",
    "            pool.apply_async(model.fit, args=(T[nt],), kwds={'obs_mask':maskT, 'max_iter':max_iter}, callback=_callback)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print('elapsed time', time.time()-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Cumulative Regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance with Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute cumulative regret for each configuration first\n",
    "summary = dict()\n",
    "#for seq, nt, n_particle, rbp, var_x, mc_move in result:\n",
    "for nt, n_particle, rbp, var_x, mc_move in itertools.product(range(n_test), n_particles, rbps, var_xs, mc_moves):\n",
    "    file = os.path.join(dest, 'pThompson_%d_%d_%d_%d_%.2f_%.2f_%.2f_%d_%r_%.2f_%d.txt' % (nt, n_dim, n_entity, n_relation, var_e, var_r, _var_x, n_particle, rbp, var_x, mc_move))\n",
    "    seq = [line.split(',') for line in open(file, 'r').readlines()]    \n",
    "    if not summary.__contains__((n_particle, rbp, var_x, mc_move)):\n",
    "        summary[(n_particle, rbp, var_x, mc_move)] = np.zeros(max_iter)\n",
    "        \n",
    "    regret = compute_regret(T[nt], seq)\n",
    "    summary[(n_particle, rbp, var_x, mc_move)] += np.cumsum(regret)\n",
    "\n",
    "\n",
    "# compute cumulative regret of random selection\n",
    "for nt in range(n_test):\n",
    "    mask = np.ones_like(T[nt])\n",
    "    _seq = [s for s in itertools.product(range(n_relation), range(n_entity), range(n_entity))]\n",
    "    np.random.shuffle(_seq)\n",
    "    regret = compute_regret(T[nt], _seq)\n",
    "    random_regret += np.cumsum(regret)\n",
    "\n",
    "# plot cumulative regrets\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(range(max_iter), random_regret/n_test, label='RANDOM')\n",
    "\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    plt.plot(range(max_iter), summary[key]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which configuration performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = list()\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    final_regret = summary[key][-1]\n",
    "    models.append((final_regret,'pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move)))\n",
    "models.sort()\n",
    "for model in models:\n",
    "    print(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Regret of First 200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot cumulative regrets\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "print_iter = 200\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    if var_x == 0.1:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(range(print_iter), summary[key][:print_iter]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "    else:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(range(print_iter), summary[key][:print_iter]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.1')\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Regret of After 200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot cumulative regrets\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "print_iter = 200\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    if var_x == 0.1:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(range(max_iter-print_iter), summary[key][print_iter:]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "    else:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(range(max_iter-print_iter), summary[key][print_iter:]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.1')\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
