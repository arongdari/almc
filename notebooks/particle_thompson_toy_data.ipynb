{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Particle Thompson Sampling with Toy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model with synthetic data and compare the cumulative regret of the model with the cumulative regret of random selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "\n",
    "#from almc.bayesian_rescal import gen_random_tensor\n",
    "from almc.bayesian_rescal import PFBayesianRescal, compute_regret\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Toy Dataset & RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 10\n"
     ]
    }
   ],
   "source": [
    "n_test = 10\n",
    "\n",
    "n_entities = [20]\n",
    "n_relations = [10]\n",
    "n_dim = 5\n",
    "n_particle = 5\n",
    "\n",
    "dest = '../result/toy2/'\n",
    "for n_entity, n_relation in itertools.product(n_entities, n_relations):\n",
    "    print(n_entity, n_relation)\n",
    "    max_iter = n_relation * n_entity**2\n",
    "\n",
    "    var_x = 0.01\n",
    "    T = gen_random_tensor(n_dim, n_entity, n_relation, var_e=1, var_r=1, var_x=var_x)\n",
    "    for nt in range(n_test):\n",
    "        log_file = os.path.join(dest, '%d_%d_%d_%d.txt' % (n_entity,n_relation,n_dim,nt))\n",
    "        if not os.path.exists(log_file):\n",
    "            pickle.dump(T, open(os.path.join(dest,'%d_%d_%d_T.pkl' % (n_entity,n_relation,n_dim)), 'wb'))\n",
    "            model = PFBayesianRescal(n_dim, var_x = var_x, n_particles=n_particle,\n",
    "                                     compute_score=False, sample_prior=False, \n",
    "                                     gibbs_init=False, sample_all=True, log=log_file)\n",
    "            seq = model.fit(T, obs_mask = np.zeros_like(T), max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "  \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(color)):    \n",
    "    r, g, b = color[i]    \n",
    "    color[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_test = 10\n",
    "n_entities = [20]\n",
    "n_relations = [10]\n",
    "n_dim = 5\n",
    "n_particle = 5\n",
    "dest = '../result/toy2/'\n",
    "\n",
    "for n_entity, n_relation in itertools.product(n_entities, n_relations):\n",
    "    max_iter = n_relation * n_entity**2\n",
    "    regret = np.zeros([n_test, max_iter])\n",
    "    random_regret = np.zeros(max_iter)\n",
    "    T = pickle.load(open(os.path.join(dest, '%d_%d_%d_T.pkl' % (n_entity,n_relation,n_dim)), 'rb'))\n",
    "\n",
    "    for nt in range(n_test):\n",
    "        _seq = [s for s in itertools.product(range(n_relation), range(n_entity), range(n_entity))]\n",
    "        np.random.shuffle(_seq)\n",
    "        random_regret += np.cumsum(compute_regret(T, _seq))\n",
    "\n",
    "        log_file = os.path.join(dest, '%d_%d_%d_%d.txt' % (n_entity,n_relation,n_dim,nt))\n",
    "        seq = [line.strip().split(',') for line in open(log_file).readlines()]\n",
    "        regret[nt] = np.cumsum(compute_regret(T, seq))\n",
    "\n",
    "    plt.figure(figsize=(3,2.4))\n",
    "    #plt.figure(figsize=(10,8))\n",
    "    plt.plot(random_regret/n_test, label='Passive', linewidth=2, color='k')\n",
    "    \n",
    "    mean = np.mean(regret, 0)\n",
    "    std = np.std(regret, 0)\n",
    "    plt.plot(mean, '--', label='PNORMAL-TS', linewidth=2, color=color[0])\n",
    "    plt.fill_between(range(max_iter), mean-std, mean+std, alpha=0.3, color=color[0])\n",
    "\n",
    "    plt.legend(loc='upper left', frameon=False)\n",
    "\n",
    "    plt.setp(plt.gca().get_legend().get_texts(), fontsize='10')\n",
    "#    plt.title('# entity = %d, # relation = %d' % (n_entity, n_relation))\n",
    "    plt.xlim((0,2000))\n",
    "    plt.ylim((0,50000))    \n",
    "    plt.locator_params(axis = 'y', nbins = 4)\n",
    "    plt.locator_params(axis = 'x', nbins = 5)\n",
    "    plt.savefig('../paper/cikm2016/images/toy_%d_%d_%d.pdf' % (n_entity,n_relation,n_dim), format='PDF', bbox_inches='tight', pad_inches=0.1)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_test = 10\n",
    "\n",
    "n_dim = 5\n",
    "n_entities = 5\n",
    "n_relations = 10\n",
    "\n",
    "#for n_dim, n_entity, n_relation in itertools.product(n_dims, n_entities, n_relations):\n",
    "    # additional parameters\n",
    "max_iter = n_relation * n_entity**2\n",
    "random_regret = np.zeros(max_iter)\n",
    "result = list()\n",
    "\n",
    "# dest\n",
    "dest = '../result/toy2/'\n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pool = mp.Pool(8)\n",
    "tic=time.time()\n",
    "\n",
    "T = np.zeros([n_test, n_relation, n_entity, n_entity])\n",
    "for nt in range(n_test):\n",
    "    # generate toy data\n",
    "    var_e = 1.\n",
    "    var_r = 1.\n",
    "    var_x = 0.01\n",
    "    test_file = os.path.join(dest, 'toy_data_%d_%d_%d_%d_%.2f_%.2f_%.2f.pkl' % (nt, n_dim, n_entity, n_relation, var_e, var_r, var_x))\n",
    "    if os.path.exists(test_file):\n",
    "        with open(test_file, 'rb') as f:\n",
    "            _T = pickle.load(f)\n",
    "            T[nt] = _T\n",
    "    else:\n",
    "        _T = gen_random_tensor(n_dim, n_entity, n_relation, var_e=var_e, var_r=var_r, var_x=var_x)\n",
    "        T[nt] = _T\n",
    "        with open(test_file, 'wb') as f:\n",
    "            pickle.dump(_T, f)\n",
    "\n",
    "    _var_x = var_x\n",
    "    # particle thompson sampling\n",
    "    for n_particle, rbp, var_x, mc_move in itertools.product(n_particles, rbps, var_xs, mc_moves):\n",
    "        log_file = os.path.join(dest, 'pThompson_%d_%d_%d_%d_%.2f_%.2f_%.2f_%d_%r_%.2f_%d.txt' % (nt, n_dim, n_entity, n_relation, var_e, var_r, _var_x, n_particle, rbp, var_x, mc_move))\n",
    "        if os.path.exists(log_file):\n",
    "            continue\n",
    "\n",
    "        def finalize(nt, n_particle, rbp, var_x, mc_move):\n",
    "            # function argument closure\n",
    "            def inner(rval):\n",
    "                return result.append((rval, nt, n_particle, rbp, var_x, mc_move))\n",
    "            return inner\n",
    "\n",
    "        _callback = finalize(nt, n_particle, rbp, var_x, mc_move)\n",
    "\n",
    "        print(log_file)\n",
    "        maskT = np.zeros_like(T[nt])\n",
    "        model = PFBayesianRescal(n_dim, var_x = var_x, n_particles=n_particle,\n",
    "                                 compute_score=False, sample_prior=False, \n",
    "                                 gibbs_init=False, rbp=rbp, log=log_file)\n",
    "        pool.apply_async(model.fit, args=(T[nt],), kwds={'obs_mask':maskT, 'max_iter':max_iter}, callback=_callback)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "print('elapsed time', time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for nt, n_particle, rbp, var_x, mc_move in itertools.product(range(n_test), n_particles, rbps, var_xs, mc_moves):\n",
    "    file = os.path.join(dest, 'pThompson_%d_%d_%d_%d_%.2f_%.2f_%.2f_%d_%r_%.2f_%d.txt' \n",
    "                        % (nt, n_dim, n_entity, n_relation, var_e, var_r, _var_x, n_particle, rbp, var_x, mc_move))\n",
    "    seq = [line.split(',') for line in open(file, 'r').readlines()]    \n",
    "    if (n_particle, rbp, var_x, mc_move) not in summary:\n",
    "        summary[(n_particle, rbp, var_x, mc_move)] = np.zeros(max_iter)\n",
    "\n",
    "    regret = compute_regret(T[nt], seq)\n",
    "    summary[(n_particle, rbp, var_x, mc_move)] += np.cumsum(regret)\n",
    "\n",
    "\n",
    "# compute cumulative regret of random selection\n",
    "random_regret = np.zeros(max_iter)\n",
    "for nt in range(n_test):\n",
    "    _seq = [s for s in itertools.product(range(n_relation), range(n_entity), range(n_entity))]\n",
    "    np.random.shuffle(_seq)\n",
    "    regret = compute_regret(T[nt], _seq)\n",
    "    random_regret += np.cumsum(regret)\n",
    "\n",
    "# plot cumulative regrets\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(range(max_iter), random_regret/n_test, label='RANDOM')\n",
    "\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    plt.plot(range(max_iter), summary[key]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Cumulative Regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance with Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute cumulative regret for each configuration first\n",
    "dest = '../result/toy2/'\n",
    "#for seq, nt, n_particle, rbp, var_x, mc_move in result:\n",
    "for n_dim, n_entity, n_relation in itertools.product(n_dims, n_entities, n_relations):\n",
    "    summary = dict()    \n",
    "    max_iter = n_relation * n_entity**2\n",
    "    \n",
    "    T = np.zeros([n_test, n_relation, n_entity, n_entity])\n",
    "    for nt in range(n_test):\n",
    "        test_file = os.path.join(dest, 'toy_data_%d_%d_%d_%d_%.2f_%.2f_%.2f.pkl' \n",
    "                                 % (nt, n_dim, n_entity, n_relation, var_e, var_r, var_x))        \n",
    "        with open(test_file, 'rb') as f:\n",
    "            _T = pickle.load(f)\n",
    "            T[nt] = _T\n",
    "                \n",
    "    for nt, n_particle, rbp, var_x, mc_move in itertools.product(range(n_test), n_particles, rbps, var_xs, mc_moves):\n",
    "        file = os.path.join(dest, 'pThompson_%d_%d_%d_%d_%.2f_%.2f_%.2f_%d_%r_%.2f_%d.txt' \n",
    "                            % (nt, n_dim, n_entity, n_relation, var_e, var_r, _var_x, n_particle, rbp, var_x, mc_move))\n",
    "        seq = [line.split(',') for line in open(file, 'r').readlines()]    \n",
    "        if (n_particle, rbp, var_x, mc_move) not in summary:\n",
    "            summary[(n_particle, rbp, var_x, mc_move)] = np.zeros(max_iter)\n",
    "\n",
    "        regret = compute_regret(T[nt], seq)\n",
    "        summary[(n_particle, rbp, var_x, mc_move)] += np.cumsum(regret)\n",
    "\n",
    "\n",
    "    # compute cumulative regret of random selection\n",
    "    random_regret = np.zeros(max_iter)\n",
    "    for nt in range(n_test):\n",
    "        _seq = [s for s in itertools.product(range(n_relation), range(n_entity), range(n_entity))]\n",
    "        np.random.shuffle(_seq)\n",
    "        regret = compute_regret(T[nt], _seq)\n",
    "        random_regret += np.cumsum(regret)\n",
    "\n",
    "    # plot cumulative regrets\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(range(max_iter), random_regret/n_test, label='RANDOM')\n",
    "\n",
    "    for key in summary.keys():\n",
    "        n_particle, rbp, var_x, mc_move = key\n",
    "        plt.plot(range(max_iter), summary[key]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "\n",
    "    plt.legend(loc=0)\n",
    "    plt.title('Cumulative Regret')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which configuration performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = list()\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    final_regret = summary[key][-1]\n",
    "    models.append((final_regret,'pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move)))\n",
    "models.sort()\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Regret of First 200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot cumulative regrets\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "print_iter = 200\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    if var_x == 0.1:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(range(print_iter), summary[key][:print_iter]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "    else:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(range(print_iter), summary[key][:print_iter]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.1')\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Regret of After 200 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot cumulative regrets\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "print_iter = 200\n",
    "for key in summary.keys():\n",
    "    n_particle, rbp, var_x, mc_move = key\n",
    "    if var_x == 0.1:\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(range(max_iter-print_iter), summary[key][print_iter:]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "    else:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(range(max_iter-print_iter), summary[key][print_iter:]/n_test, label='pThompson-%d_%r_%.2f_%d' % (n_particle, rbp, var_x, mc_move))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.1')\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(loc=0)\n",
    "plt.title('Cumulative Regret, var_x = 0.01')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
